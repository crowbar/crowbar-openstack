[DEFAULT]

#
# From oslo.log
#

# If set to true, the logging level will be set to DEBUG instead of the default INFO
# level. (boolean value)
# Note: This option can be changed without restarting.
#debug = false

# The name of a logging configuration file. This file is appended to any existing logging
# configuration files. For details about logging configuration files, see the Python
# logging module documentation. Note that when logging configuration files are used then
# all logging configuration is set in the configuration file and other logging
# configuration options are ignored (for example, logging_context_format_string). (string
# value)
# Note: This option can be changed without restarting.
# Deprecated group/name - [DEFAULT]/log_config
#log_config_append = <None>

# Defines the format string for %%(asctime)s in log records. Default: %(default)s . This
# option is ignored if log_config_append is set. (string value)
#log_date_format = %Y-%m-%d %H:%M:%S

# (Optional) Name of log file to send logging output to. If no default is set, logging
# will go to stderr as defined by use_stderr. This option is ignored if log_config_append
# is set. (string value)
# Deprecated group/name - [DEFAULT]/logfile
#log_file = <None>

# (Optional) The base directory used for relative log_file  paths. This option is ignored
# if log_config_append is set. (string value)
# Deprecated group/name - [DEFAULT]/logdir
#log_dir = <None>
log_dir = /var/log/monasca-persister/

# Uses logging handler designed to watch file system. When log file is moved or removed
# this handler will open a new log file with specified path instantaneously. It makes
# sense only if log_file option is specified and Linux platform is used. This option is
# ignored if log_config_append is set. (boolean value)
#watch_log_file = false

# Use syslog for logging. Existing syslog format is DEPRECATED and will be changed later
# to honor RFC5424. This option is ignored if log_config_append is set. (boolean value)
#use_syslog = false

# Enable journald for logging. If running in a systemd environment you may wish to enable
# journal support. Doing so will use the journal native protocol which includes structured
# metadata in addition to log messages.This option is ignored if log_config_append is set.
# (boolean value)
#use_journal = false

# Syslog facility to receive log lines. This option is ignored if log_config_append is
# set. (string value)
#syslog_log_facility = LOG_USER

# Use JSON formatting for logging. This option is ignored if log_config_append is set.
# (boolean value)
#use_json = false

# Log output to standard error. This option is ignored if log_config_append is set.
# (boolean value)
#use_stderr = false

# Format string to use for log messages with context. (string value)
#logging_context_format_string = %(asctime)s.%(msecs)03d %(process)d %(levelname)s %(name)s [%(request_id)s %(user_identity)s] %(instance)s%(message)s

# Format string to use for log messages when context is undefined. (string value)
#logging_default_format_string = %(asctime)s.%(msecs)03d %(process)d %(levelname)s %(name)s [-] %(instance)s%(message)s

# Additional data to append to log message when logging level for the message is DEBUG.
# (string value)
#logging_debug_format_suffix = %(funcName)s %(pathname)s:%(lineno)d

# Prefix each line of exception output with this format. (string value)
#logging_exception_prefix = %(asctime)s.%(msecs)03d %(process)d ERROR %(name)s %(instance)s

# Defines the format string for %(user_identity)s that is used in
# logging_context_format_string. (string value)
#logging_user_identity_format = %(user)s %(tenant)s %(domain)s %(user_domain)s %(project_domain)s

# List of package logging levels in logger=LEVEL pairs. This option is ignored if
# log_config_append is set. (list value)
#default_log_levels = amqp=WARN,amqplib=WARN,boto=WARN,qpid=WARN,sqlalchemy=WARN,suds=INFO,oslo.messaging=INFO,oslo_messaging=INFO,iso8601=WARN,requests.packages.urllib3.connectionpool=WARN,urllib3.connectionpool=WARN,websocket=WARN,requests.packages.urllib3.util.retry=WARN,urllib3.util.retry=WARN,keystonemiddleware=WARN,routes.middleware=WARN,stevedore=WARN,taskflow=WARN,keystoneauth=WARN,oslo.cache=INFO,dogpile.core.dogpile=INFO

# Enables or disables publication of error events. (boolean value)
#publish_errors = false

# The format for an instance that is passed with the log message. (string value)
#instance_format = "[instance: %(uuid)s] "

# The format for an instance UUID that is passed with the log message. (string value)
#instance_uuid_format = "[instance: %(uuid)s] "

# Interval, number of seconds, of log rate limiting. (integer value)
#rate_limit_interval = 0

# Maximum number of logged messages per rate_limit_interval. (integer value)
#rate_limit_burst = 0

# Log level name used by rate limiting: CRITICAL, ERROR, INFO, WARNING, DEBUG or empty
# string. Logs with level greater or equal to rate_limit_except_level are not filtered. An
# empty string means that all levels are filtered. (string value)
#rate_limit_except_level = CRITICAL

# Enables or disables fatal status of deprecations. (boolean value)
#fatal_deprecations = false


[cassandra]

#
# From monasca_persister
#

# Comma separated list of Cassandra node IP addresses (list value)
#contact_points = 127.0.0.1

# Cassandra port number (integer value)
#port = 8086

# Keyspace name where metrics are stored (string value)
#keyspace = monasca

# Cassandra user name (string value)
#user =

# Cassandra password (string value)
#password =

# Cassandra timeout in seconds when creating a new connection (integer value)
#connection_timeout = 5

# Cassandra read timeout in seconds (integer value)
#read_timeout = 60

# Maximum number of retries in write ops (integer value)
#max_write_retries = 1

# Maximum number of cached metric definition entries in memory (integer value)
#max_definition_cache_size = 20000000

# Data retention period in days (integer value)
#retention_policy = 45

# Cassandra default consistency level (string value)
#consistency_level = ONE

# Cassandra local data center name (string value)
#local_data_center = <None>

# Maximum batch size in Cassandra (integer value)
#max_batches = 250


[elasticsearch]

#
# From monasca_persister
#

# Index name where events are stored (string value)
#index_name = monevents

# List of Elasticsearch nodes in format host[:port] (list value)
#hosts = localhost:9200

# Flag indicating whether to obtain a list of nodes from the cluser at startup time
# (boolean value)
#sniff_on_start = false

# Flag controlling if connection failure triggers a sniff (boolean value)
#sniff_on_connection_fail = false

# Number of seconds between automatic sniffs (integer value)
#sniffer_timeout = <None>

# Maximum number of retries before an exception is propagated (integer value)
# Minimum value: 1
#max_retries = 3


[influxdb]

#
# From monasca_persister
#

# database name where metrics are stored (string value)
#database_name = mon
database_name = <%= node['monasca']['db_monapi']['database'] %>

# Valid IP address or hostname to InfluxDB instance (host address value)
#ip_address = <None>
ip_address = <%= @influxdb_host %>

# port to influxdb (port value)
# Minimum value: 0
# Maximum value: 65535
#port = 8086
port = <%= node['monasca']['influxdb']['client_port'] %>

# influxdb user  (string value)
#user = mon_persister
user = <%= node['monasca']['persister']['influxdb_user'] %>

# influxdb password (string value)
#password = <None>
password = <%= node['monasca']['master']['tsdb_mon_persister_password'] %>


[kafka]

#
# From monasca_persister
#

# Maximum wait time for write batch to database (integer value)
#max_wait_time_seconds = 30

# Fetch size, in bytes. This value is set to the kafka-python defaults (integer value)
#fetch_size_bytes = 4096

# Buffer size, in bytes. This value is set to the kafka-python defaults (integer value)
#buffer_size = $kafka.fetch_size_bytes

# Maximum buffer size, in bytes, default value is 8 time buffer_size.This value is set to
# the kafka-python defaults.  (integer value)
#max_buffer_size = 32768

# Number of processes spawned by persister (integer value)
#num_processors = 0
num_processors = 1

# Name/id of persister kafka consumer (string value)
# Advanced Option: intended for advanced users and not used
# by the majority of users, and might have a significant
# effect on stability and/or performance.
#consumer_id = monasca-persister

# id of persister kafka client (string value)
# Advanced Option: intended for advanced users and not used
# by the majority of users, and might have a significant
# effect on stability and/or performance.
#client_id = monasca-persister


[kafka_alarm_history]

#
# From monasca_persister
#

# Enable alarm state history persister (boolean value)
#enabled = true

# Comma separated list of Kafka broker host:port (list value)
#uri = 127.0.0.1:9092
uri=<%= @kafka_host %>:<%= node[:monasca][:kafka][:port] %>

# Kafka Group from which persister get data (string value)
#group_id = 1_alarm-state-transitions

# Kafka Topic from which persister get data (string value)
#topic = alarm-state-transitions

# Path in zookeeper for kafka consumer group partitioning algorithm (string value)
#zookeeper_path = /persister_partitions/$kafka_alarm_history.topic

# Maximum number of alarm state history messages to buffer before writing to database
# (integer value)
#batch_size = 1

# Maximum wait time for write batch to database (integer value)
#max_wait_time_seconds = $kafka.max_wait_time_seconds

# Fetch size, in bytes. This value is set to the kafka-python defaults (integer value)
#fetch_size_bytes = $kafka.fetch_size_bytes

# Buffer size, in bytes. This value is set to the kafka-python defaults (integer value)
#buffer_size = $kafka.buffer_size

# Maximum buffer size, in bytes, default value is 8 time buffer_size.This value is set to
# the kafka-python defaults.  (integer value)
#max_buffer_size = $kafka.max_buffer_size

# Number of processes spawned by persister (integer value)
#num_processors = $kafka.num_processors

# Name/id of persister kafka consumer (string value)
# Advanced Option: intended for advanced users and not used
# by the majority of users, and might have a significant
# effect on stability and/or performance.
#consumer_id = $kafka.consumer_id

# id of persister kafka client (string value)
# Advanced Option: intended for advanced users and not used
# by the majority of users, and might have a significant
# effect on stability and/or performance.
#client_id = $kafka.client_id


[kafka_events]

#
# From monasca_persister
#

# Enable event persister (boolean value)
#enabled = false

# Comma separated list of Kafka broker host:port (list value)
#uri = 127.0.0.1:9092

# Kafka Group from which persister get data (string value)
#group_id = 1_events

# Kafka Topic from which persister get data (string value)
#topic = monevents

# Path in zookeeper for kafka consumer group partitioning algorithm (string value)
#zookeeper_path = /persister_partitions/$kafka_events.topic

# Maximum wait time for write batch to database (integer value)
#max_wait_time_seconds = $kafka.max_wait_time_seconds

# Fetch size, in bytes. This value is set to the kafka-python defaults (integer value)
#fetch_size_bytes = $kafka.fetch_size_bytes

# Buffer size, in bytes. This value is set to the kafka-python defaults (integer value)
#buffer_size = $kafka.buffer_size

# Maximum buffer size, in bytes, default value is 8 time buffer_size.This value is set to
# the kafka-python defaults.  (integer value)
#max_buffer_size = $kafka.max_buffer_size

# Number of processes spawned by persister (integer value)
#num_processors = $kafka.num_processors

# Name/id of persister kafka consumer (string value)
# Advanced Option: intended for advanced users and not used
# by the majority of users, and might have a significant
# effect on stability and/or performance.
#consumer_id = $kafka.consumer_id

# id of persister kafka client (string value)
# Advanced Option: intended for advanced users and not used
# by the majority of users, and might have a significant
# effect on stability and/or performance.
#client_id = $kafka.client_id


[kafka_metrics]

#
# From monasca_persister
#

# Enable metrics persister (boolean value)
#enabled = true

# Comma separated list of Kafka broker host:port (list value)
#uri = 127.0.0.1:9092

# Kafka Group from which persister get data (string value)
#group_id = 1_metrics

# Kafka Topic from which persister get data (string value)
#topic = metrics

# Path in zookeeper for kafka consumer group partitioning algorithm (string value)
#zookeeper_path = /persister_partitions/$kafka_metrics.topic

# Maximum number of metrics to buffer before writing to database (integer value)
#batch_size = 20000

# Maximum wait time for write batch to database (integer value)
#max_wait_time_seconds = $kafka.max_wait_time_seconds

# Fetch size, in bytes. This value is set to the kafka-python defaults (integer value)
#fetch_size_bytes = $kafka.fetch_size_bytes

# Buffer size, in bytes. This value is set to the kafka-python defaults (integer value)
#buffer_size = $kafka.buffer_size

# Maximum buffer size, in bytes, default value is 8 time buffer_size.This value is set to
# the kafka-python defaults.  (integer value)
#max_buffer_size = $kafka.max_buffer_size

# Number of processes spawned by persister (integer value)
#num_processors = $kafka.num_processors

# Name/id of persister kafka consumer (string value)
# Advanced Option: intended for advanced users and not used
# by the majority of users, and might have a significant
# effect on stability and/or performance.
#consumer_id = $kafka.consumer_id

# id of persister kafka client (string value)
# Advanced Option: intended for advanced users and not used
# by the majority of users, and might have a significant
# effect on stability and/or performance.
#client_id = $kafka.client_id


[repositories]

#
# From monasca_persister
#

# The repository driver to use for metrics (string value)
#metrics_driver = monasca_persister.repositories.influxdb.metrics_repository:MetricInfluxdbRepository

# The repository driver to use for alarm state history (string value)
#alarm_state_history_driver = monasca_persister.repositories.influxdb.metrics_repository:MetricInfluxdbRepository

# The repository driver to use for events (string value)
#events_driver = monasca_persister.repositories.elasticsearch.events_repository:ElasticSearchEventsRepository


[zookeeper]

#
# From monasca_persister
#

# Comma separated list of zookeper instance host:port (list value)
#uri = 127.0.0.1:2181
uri=<%= @zookeeper_host %>:<%= node[:monasca][:zookeeper][:client_port] %>
# Time between rechecking if partition is available (integer value)
#partition_interval_recheck_seconds = 15
